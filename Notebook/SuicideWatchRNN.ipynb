{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.io as pio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['wife', 'threaten', 'suicide', 'recently', 'l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['weird', 'get', 'affect', 'come', 'someone', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['need', 'help', 'just', 'help', 'cry', 'hard']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['lose', 'hello', 'name', 'struggle', 'year', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ['wife', 'threaten', 'suicide', 'recently', 'l...      1\n",
       "1  ['weird', 'get', 'affect', 'come', 'someone', ...      0\n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...      0\n",
       "3    ['need', 'help', 'just', 'help', 'cry', 'hard']      1\n",
       "4  ['lose', 'hello', 'name', 'struggle', 'year', ...      1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"C:/Users/ryan_/OneDrive/Desktop/SuicideWatchDetection/src/Data/cleaned_data.csv\"\n",
    "\n",
    "# Define column names based on the first line of the file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "data.rename(columns={'class': 'label'}, inplace=True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Define Custom Dataset Class\n",
    "class CustomDataset():\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx, 0]  # Assuming text is in the first column\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming label is in the second column\n",
    "        return text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "1  ['wife', 'threaten', 'suicide', 'recently', 'l...     1\n",
      "2  ['weird', 'get', 'affect', 'come', 'someone', ...     0\n",
      "3  ['finally', 'almost', 'never', 'hear', 'bad', ...     0\n",
      "4    ['need', 'help', 'just', 'help', 'cry', 'hard']     1\n",
      "5  ['lose', 'hello', 'name', 'struggle', 'year', ...     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file into a DataFrame, specifying the delimiter\n",
    "df = pd.read_csv(r'C:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\src\\Data\\cleaned_data.csv', delimiter=',', header=None)  # Assuming no header\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "# Rename the class column to \"label\"\n",
    "df = df.rename(columns={0: 'text', 1: 'label'})\n",
    "df = df.drop(0)\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_valid_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, valid_df = train_test_split(train_valid_df, test_size=0.25, random_state=42)  # Splitting 80% for train and 20% for validation\n",
    "\n",
    "# Process tokens to remove extra characters (such as commas, single quotes, and square brackets)\n",
    "def preprocess_tokens(sentence):\n",
    "    return [word.strip(\",'[]\") for word in sentence.split()]\n",
    "\n",
    "# Create a counter to count word frequencies\n",
    "word_counter = Counter()\n",
    "\n",
    "# Count word frequencies from the training data\n",
    "for sentence in train_df['text']:\n",
    "    word_counter.update(preprocess_tokens(sentence))\n",
    "\n",
    "# Create a vocabulary ordered by word frequency\n",
    "vocab_ordered = {'<pad>': 0, '<unk>': 1}\n",
    "word_index = len(vocab_ordered)\n",
    "for word, count in word_counter.most_common():\n",
    "    vocab_ordered[word] = word_index\n",
    "    word_index += 1\n",
    "\n",
    "# Optionally, you can save the vocab to a file for later use\n",
    "with open('vocab_ordered.txt', 'w') as f:\n",
    "    for word, index in vocab_ordered.items():\n",
    "        f.write(f\"{word}: {index}\\n\")\n",
    "\n",
    "# Now you have train_df, valid_df, test_df DataFrames with the label column renamed\n",
    "# You can proceed with tokenization, building vocabulary, and encoding labels as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS THE PROBLEM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS FOR Today:\n",
    "### Build 3 RNN Models using these three types of recurrent layers\n",
    "#### RNN\n",
    "#### LSTM\n",
    "#### GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define text and label pipelines\n",
    "text_pipeline = lambda x: [vocab_ordered[word] if word in vocab_ordered else vocab_ordered['<unk>'] for word in preprocess_tokens(x)]\n",
    "label_pipeline = lambda x: torch.tensor([int(x), 1 - int(x)], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "    label_list = torch.stack(label_list, dim = 0)\n",
    "   \n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create a dictionary containing the processed text, labels, and lengths\n",
    "    batch_dict = {\n",
    "        'text': padded_text_list,\n",
    "        'label': label_list\n",
    "    }\n",
    "       \n",
    "    return batch_dict\n",
    "\n",
    "# Assuming you have already defined your training, validation, and test DataFrames as train_df, valid_df, and test_df respectively\n",
    "\n",
    "# Instantiate CustomDataset objects\n",
    "train_dataset = CustomDataset(train_df)\n",
    "valid_dataset = CustomDataset(valid_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 15\n",
    "\n",
    "# Create DataLoader objects for training, validation, and test sets\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch sizes of DataLoader objects\n",
    "def check_batch_sizes(dataloader):\n",
    "    batch_sizes = []\n",
    "    for batch in dataloader:\n",
    "        batch_sizes.append(batch['text'].shape[0])# Get batch size from the 'text' tensor\n",
    "        x = batch_sizes\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    print(batch['text'].dtype)\n",
    "    print(batch['label'].dtype)\n",
    "    break\n",
    "      # Only print the structure of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "       \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           batch_first=True)  # LSTM layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)  # Global max pooling\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_output, hidden = self.lstm(embedded)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        out = self.fc(lstm_output[:, -1])  # Apply linear layer to last output\n",
    "        output = torch.sigmoid(out)  # Apply sigmoid activation\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        '''Initialize Hidden State'''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            return hidden\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_ordered)+1\n",
    "output_size = 2\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net = LSTMModel(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    # training params\n",
    "    epochs = 2\n",
    "    counter = 0\n",
    "    print_every = 500\n",
    "    clip=5 # gradient clipping\n",
    "    print(\"Epochs:\", epochs)\n",
    "    print(\"Print every:\", print_every)\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        # batch loop\n",
    "        for batch in train_dl:\n",
    "            counter += 1\n",
    "            text, labels = batch['text'], batch['label']\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                text, labels = text.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = net.init_hidden(batch_size)\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(text, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for val_batch in valid_dl:\n",
    "                    val_text, val_labels = val_batch['text'], val_batch['label']\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = net.init_hidden(batch_size)\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if train_on_gpu:\n",
    "                        val_text, val_labels = val_text.cuda(), val_labels.cuda()\n",
    "\n",
    "                    output, val_h = net(val_text, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), val_labels)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                   \n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2\n",
      "Print every: 500\n",
      "Epoch: 1/2... Step: 500... Loss: 0.661431... Val Loss: 0.675299\n",
      "Epoch: 1/2... Step: 1000... Loss: 0.330012... Val Loss: 0.358360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[223], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_on_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[222], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_on_gpu:\n\u001b[0;32m     56\u001b[0m     val_text, val_labels \u001b[38;5;241m=\u001b[39m val_text\u001b[38;5;241m.\u001b[39mcuda(), val_labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 58\u001b[0m output, val_h \u001b[38;5;241m=\u001b[39m net(val_text, val_h)\n\u001b[0;32m     59\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), val_labels)\n\u001b[0;32m     61\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[220], line 31\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     30\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m---> 31\u001b[0m lstm_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded)\n\u001b[0;32m     32\u001b[0m lstm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(lstm_output)\n\u001b[0;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Apply linear layer to last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryan_\\OneDrive\\Desktop\\SuicideWatchDetection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    879\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_gpu = False\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size)\n",
    "print(embedding_dim)\n",
    "print(hidden_size)\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_size)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the training loop\n",
    "# Define the training loop\n",
    "num_epochs = \n",
    "counter = 0 \n",
    "print_every = 1000\n",
    "clip = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to train mode\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dl:\n",
    "        inputs, targets = batch['text'].to(torch.long), batch['label'] # Unpack the batch\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), targets.float())  # Calculate the loss\n",
    "        print(\"Output Shape:\", outputs.shape)\n",
    "        print(\"Target Shape:\", targets.shape)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the parameters\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {average_loss}\")\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        for batch in valid_dl:\n",
    "            inputs, targets = batch['text'].to(torch.long), batch['label'] # Unpack the batch\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs.squeeze(), targets.float())  # Calculate the loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        average_valid_loss = valid_loss / len(valid_dl.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {average_valid_loss}\")\n",
    "\n",
    "# After training, you can evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for batch in test_dl:\n",
    "        inputs, targets = batch['text'].to(torch.long), batch['label'] # Unpack the batch\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), targets.float())  # Calculate the loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average test loss\n",
    "    average_test_loss = test_loss / len(test_dl.dataset)\n",
    "    print(f\"Test Loss: {average_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Random Initial Weights\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wxy = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wyz = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.Bh = np.zeros((hidden_size, 1))\n",
    "        self.Bo = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = None\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        self.a = np.dot(self.Wx, inputs) + np.dot(self.Wxy, h_prev) + self.Bh\n",
    "        self.h = np.tanh(self.a)\n",
    "        self.y = np.dot(self.Wyz, self.h) + self.Bo\n",
    "        return self.y, self.h\n",
    "\n",
    "    def backward(self, d_y, h, h_prev, inputs):\n",
    "        d_Wyz = np.dot(d_y, self.h.T)\n",
    "        d_Bo = d_y\n",
    "        d_h = np.dot(self.Wyz.T, d_y)\n",
    "        d_a = np.dot(1 - self.h ** 2, d_h)\n",
    "        d_Bh = d_a\n",
    "        d_Wxy = np.dot(d_a, h_prev.T)\n",
    "        d_Wx = np.dot(d_a, inputs.T)\n",
    "        d_h_prev = np.dot(self.Wxy.T, d_a)\n",
    "        \n",
    "        gradients = {\n",
    "            'd_Wx': d_Wx,\n",
    "            'd_Wxy': d_Wxy,\n",
    "            'd_Wyz': d_Wyz,\n",
    "            'd_Bh': d_Bh,\n",
    "            'd_Bo': d_Bo,\n",
    "            'd_h_prev': d_h_prev\n",
    "        }\n",
    "        return gradients\n",
    "    \n",
    "    def train_step(self, inputs, targets, h_prev):\n",
    "        # Forward pass\n",
    "        y_pred, hidden_state = self.forward(inputs, h_prev)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = np.mean((y_pred - targets)**2)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = self.backward(2 * (y_pred - targets), hidden_state, h_prev, inputs)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.step(self, gradients)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(dataloader):  \n",
    "    total_acc, total_loss = 0.0, 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f\"Batch Index: {i}\")\n",
    "        text_batch = batch['text']\n",
    "        label_batch = batch['label']\n",
    "        lengths = batch['length']\n",
    "        \n",
    "        num_classes = 2  # Number of classes in your classification task\n",
    "        label_batch_one_hot = torch.zeros(label_batch.size(0), num_classes, dtype=torch.float32)  # Initialize with float dtype\n",
    "        label_batch_one_hot.scatter_(1, label_batch.unsqueeze(1).long(), 1)  # Convert label_batch to int64 using .long()\n",
    "        \n",
    "        h_prev = np.zeros((hidden_size, text_batch.shape[1]))  # Initialize hidden state\n",
    "        print(\"Initial h_prev:\", h_prev)\n",
    "        loss = rnn_model.train_step(text_batch, label_batch, h_prev)\n",
    "        print(\"Updated h_prev:\", h_prev)\n",
    "        \n",
    "        total_loss += loss * text_batch.shape[1]\n",
    "        \n",
    "        pred = rnn_model.forward(text_batch, h_prev)[0]\n",
    "        total_acc += ((pred >= 0.5).astype(float) == label_batch).astype(float).sum().item()\n",
    "        \n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):  \n",
    "    total_acc, total_loss = 0.0, 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f\"Batch Index: {i}\")\n",
    "        text_batch = batch['text']\n",
    "        label_batch = batch['label']\n",
    "        lengths = batch['length']\n",
    "        \n",
    "        h_prev = np.zeros((hidden_size, text_batch.shape[1]))  # Initialize hidden state\n",
    "     # Initialize hidden state\n",
    "        \n",
    "        loss = rnn_model.train_step(text_batch, label_batch, h_prev)\n",
    "        \n",
    "        total_loss += loss * text_batch.shape[0]\n",
    "        \n",
    "        pred = rnn_model.forward(text_batch, h_prev)[0]\n",
    "        total_acc += ((pred >= 0.5).astype(float) == label_batch).astype(float).sum().item()\n",
    "        \n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train_model(train_dl)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
